import os
import json
import asyncio
import random
import re
import time
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple

import requests
import feedparser
import urllib.parse
from bs4 import BeautifulSoup
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from openai import OpenAI

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å Copilot SDK (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
try:
    from github_copilot_sdk import CopilotClient
    COPILOT_SDK_AVAILABLE = True
except ImportError:
    COPILOT_SDK_AVAILABLE = False
    print("‚ö†Ô∏è GitHub Copilot SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π OpenAI API")

# ============ CONFIG ============

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
USE_COPILOT_SDK = os.getenv("USE_COPILOT_SDK", "false").lower() == "true"

if not all([OPENAI_API_KEY, TELEGRAM_BOT_TOKEN, CHANNEL_ID]):
    raise ValueError("‚ùå –ù–µ –≤—Å–µ ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")

bot = Bot(
    token=TELEGRAM_BOT_TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML),
)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Copilot SDK –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –≤–∫–ª—é—á–µ–Ω
copilot_client = None
if COPILOT_SDK_AVAILABLE and USE_COPILOT_SDK:
    try:
        copilot_client = CopilotClient()
        print("‚úÖ GitHub Copilot SDK –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Copilot SDK: {e}")
        print("   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π OpenAI API")

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
}

CACHE_DIR = os.getenv("CACHE_DIR", "cache")
os.makedirs(CACHE_DIR, exist_ok=True)
STATE_FILE = os.path.join(CACHE_DIR, "state.json")

RETENTION_DAYS = 14
MAX_ARTICLE_AGE_DAYS = 3
TELEGRAM_CAPTION_LIMIT = 1024

# ============ RSS –ò–°–¢–û–ß–ù–ò–ö–ò ============

RSS_SOURCES = [
    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
    {"name": "SecurityLab", "url": "https://www.securitylab.ru/rss/allnews/", "category": "security"},
    {"name": "AntiMalware", "url": "https://www.anti-malware.ru/news/feed", "category": "security"},
    
    # AI/Tech
    {"name": "Habr AI", "url": "https://habr.com/ru/rss/hub/artificial_intelligence/all/?fl=ru", "category": "ai"},
    {"name": "Habr ML", "url": "https://habr.com/ru/rss/hub/machine_learning/all/?fl=ru", "category": "ai"},
    {"name": "Habr News", "url": "https://habr.com/ru/rss/news/?fl=ru", "category": "tech"},
    
    # –†–æ—Å—Å–∏–π—Å–∫–∏–µ IT –Ω–æ–≤–æ—Å—Ç–∏
    {"name": "CNews", "url": "https://www.cnews.ru/inc/rss/news.xml", "category": "tech_ru"},
    {"name": "3DNews", "url": "https://3dnews.ru/news/rss/", "category": "tech_ru"},
    {"name": "iXBT", "url": "https://www.ixbt.com/export/news.rss", "category": "tech_ru"},
]

# ============ –û–°–ù–û–í–ù–û–ô –§–û–†–ú–ê–¢ –ü–û–°–¢–ê ============

POST_FORMAT = {
    "system": """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –ª—é–¥–µ–π. –ü–∏—à–µ—à—å –ø–æ–Ω—è—Ç–Ω–æ –∏ –ø–æ –¥–µ–ª—É.

–¢–í–û–Ø –ó–ê–î–ê–ß–ê: –≤–∑—è—Ç—å –Ω–æ–≤–æ—Å—Ç—å –æ–± —É–≥—Ä–æ–∑–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –æ–±—ä—è—Å–Ω–∏—Ç—å —á–µ–ª–æ–≤–µ–∫—É –ß–¢–û –î–ï–õ–ê–¢–¨.

–ß–ò–¢–ê–¢–ï–õ–¨: –æ–±—ã—á–Ω—ã–π —á–µ–ª–æ–≤–µ–∫ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–æ–º/–∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º, –Ω–µ —Ç–µ—Ö–Ω–∞—Ä—å.
–û–Ω —Ö–æ—á–µ—Ç: —É–∑–Ω–∞—Ç—å –æ–± —É–≥—Ä–æ–∑–µ + –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ç–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –∑–∞—â–∏—Ç—ã.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –ï—Å–ª–∏ –≤ –Ω–æ–≤–æ—Å—Ç–∏ –ï–°–¢–¨ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ‚Äî –Ω–∞–ø–∏—à–∏ –∏—Ö –ø–æ—à–∞–≥–æ–≤–æ
- –ï—Å–ª–∏ –≤ –Ω–æ–≤–æ—Å—Ç–∏ –ù–ï–¢ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π ‚Äî –ø—Ä–æ—Å—Ç–æ –æ–±—ä—è—Å–Ω–∏ —É–≥—Ä–æ–∑—É, –±–µ–∑ –≤—ã–¥—É–º—ã–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
- –ù–ï –í–´–î–£–ú–´–í–ê–ô —à–∞–≥–∏ —Ç–∏–ø–∞ "–æ—Ç–∫—Ä–æ–π—Ç–µ App Store", –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –Ω–∞–ø–∏—Å–∞–Ω–æ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–µ""",

    "template": """–ù–∞–ø–∏—à–∏ –ø–æ—Å—Ç –¥–ª—è Telegram-–∫–∞–Ω–∞–ª–∞ –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.

–°–ù–ê–ß–ê–õ–ê: –ø—Ä–æ–≤–µ—Ä—å, –µ—Å—Ç—å –ª–∏ –≤ –Ω–æ–≤–æ—Å—Ç–∏ –ö–û–ù–ö–†–ï–¢–ù–´–ï —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏/—Ä–µ—à–µ–Ω–∏–µ?

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
–ï–°–õ–ò –ï–°–¢–¨ –†–ï–®–ï–ù–ò–ï:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚ö†Ô∏è [–ó–ê–ì–û–õ–û–í–û–ö: —Å—É—Ç—å —É–≥—Ä–æ–∑—ã –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π]

**–£–≥—Ä–æ–∑–∞:**
2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è ‚Äî —á—Ç–æ —Å–ª—É—á–∏–ª–æ—Å—å, –≤ —á—ë–º –æ–ø–∞—Å–Ω–æ—Å—Ç—å.

**–ö–æ–≥–æ –∫–∞—Å–∞–µ—Ç—Å—è:**
–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ: –∫–∞–∫–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞/–ø—Ä–æ–≥—Ä–∞–º–º—ã/–≤–µ—Ä—Å–∏–∏.

**–ß—Ç–æ –¥–µ–ª–∞—Ç—å –°–ï–ô–ß–ê–°:**
1. [–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —à–∞–≥ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞]
2. [–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —à–∞–≥ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞]
3. [–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —à–∞–≥ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞]

[–¢–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ —à–∞–≥–∏ –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏!]

‚è± –ó–∞–π–º—ë—Ç: [–≤—Ä–µ–º—è]

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
–ï–°–õ–ò –ù–ï–¢ –†–ï–®–ï–ù–ò–Ø:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚ö†Ô∏è [–ó–ê–ì–û–õ–û–í–û–ö: —Å—É—Ç—å —É–≥—Ä–æ–∑—ã –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π]

**–ß—Ç–æ —Å–ª—É—á–∏–ª–æ—Å—å:**
3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è ‚Äî –æ–ø–∏—à–∏ –ø—Ä–æ–±–ª–µ–º—É –ø–æ–Ω—è—Ç–Ω—ã–º —è–∑—ã–∫–æ–º.

**–ö–æ–≥–æ –∫–∞—Å–∞–µ—Ç—Å—è:**
–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ: –∫–∞–∫–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞/–ø—Ä–æ–≥—Ä–∞–º–º—ã/–≤–µ—Ä—Å–∏–∏.

**–ß—Ç–æ –∏–∑–≤–µ—Å—Ç–Ω–æ:**
- –ú–∞—Å—à—Ç–∞–± –ø—Ä–æ–±–ª–µ–º—ã (—Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç—Ä–∞–¥–∞–ª–æ)
- –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å (–ø–∞—Ç—á –≤—ã—à–µ–ª? —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç?)
- –ù–∞ —á—Ç–æ –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ

**–°–ª–µ–¥–∏—Ç–µ –∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏** ‚Äî –∫–∞–∫ —Ç–æ–ª—å–∫–æ –ø–æ—è–≤–∏—Ç—Å—è —Ä–µ—à–µ–Ω–∏–µ, —Ä–∞—Å—Å–∫–∞–∂–µ–º.

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

–ü–†–ê–í–ò–õ–ê:
- –û–±—ä—ë–º: 600-1000 —Å–∏–º–≤–æ–ª–æ–≤
- –¢–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
- –ü—Ä–æ—Å—Ç–æ–π —è–∑—ã–∫ –±–µ–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∂–∞—Ä–≥–æ–Ω–∞
- 1-2 —ç–º–æ–¥–∑–∏ –º–∞–∫—Å–∏–º—É–º
- –ù–∏–∫–∞–∫–∏—Ö –≤—ã–¥—É–º–∞–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"""
}

# ============ –§–ò–õ–¨–¢–†–´ ============

EXCLUDE_KEYWORDS = [
    # –ë–∏–∑–Ω–µ—Å –∏ HR
    "–∞–∫—Ü–∏–∏", "–±–∏—Ä–∂–∞", "–∏–Ω–≤–µ—Å—Ç–∏—Ü", "ipo", "–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü",
    "–≤—ã—Ä—É—á–∫–∞", "–ø—Ä–∏–±—ã–ª—å", "–Ω–∞–∑–Ω–∞—á–µ–Ω", "–æ—Ç—Å—Ç–∞–≤–∫–∞", "ceo",
    "hr", "–∫–∞–¥—Ä", "–ø–µ—Ä—Å–æ–Ω–∞–ª", "—Å–æ—Ç—Ä—É–¥–Ω–∏–∫", "–∫–æ–º–ø–∞–Ω–∏",
    "–±–∏–∑–Ω–µ—Å", "–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç", "—É–ø—Ä–∞–≤–ª–µ–Ω", "—Ä–µ–∑–µ—Ä–≤", "—Ä–µ–∫—Ä—É—Ç–∏–Ω–≥",
    "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥", "–ø—Ä–æ–¥–∞–∂", "—Å—Ç—Ä–∞—Ç–µ–≥",
    
    # –†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è
    "—Ñ—É—Ç–±–æ–ª", "—Ö–æ–∫–∫–µ–π", "—Å–ø–æ—Ä—Ç", "—á–µ–º–ø–∏–æ–Ω–∞—Ç",
    "playstation", "xbox", "–≤–∏–¥–µ–æ–∏–≥—Ä",
    "–∫–∏–Ω–æ", "—Ñ–∏–ª—å–º", "—Å–µ—Ä–∏–∞–ª", "netflix",
    
    # –ü–æ–ª–∏—Ç–∏–∫–∞
    "–≤—ã–±–æ—Ä—ã", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–ø–æ–ª–∏—Ç–∏–∫", "—Å–∞–Ω–∫—Ü–∏–∏",
    
    # –ö—Ä–∏–ø—Ç–∞
    "bitcoin", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç", "nft",
    
    # –Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ
    "—Å—É–¥", "–∞—Ä–µ—Å—Ç", "–ø—Ä–∏–≥–æ–≤–æ—Ä"
]

SOURCE_PROMO_PATTERNS = [
    r"—Å–∫–∏–¥–∫[–∞–∏]", r"–ø—Ä–æ–º–æ–∫–æ–¥", r"–∞–∫—Ü–∏—è\b", r"—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞",
    r"—Ç–æ–ª—å–∫–æ —Å–µ–≥–æ–¥–Ω—è", r"—É—Å–ø–µ–π", r"–ø—Ä–µ–¥–∑–∞–∫–∞–∑",
    r"—Ü–µ–Ω–∞ –æ—Ç", r"‚ÇΩ\d+", r"\$\d+", r"‚Ç¨\d+",
]

def is_excluded(title: str, summary: str) -> Tuple[bool, str]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–∫–ª—é—á–∏—Ç—å —Å—Ç–∞—Ç—å—é"""
    text = f"{title} {summary}".lower()
    
    for kw in EXCLUDE_KEYWORDS:
        if kw in text:
            return True, f"excluded: {kw}"
    
    for pattern in SOURCE_PROMO_PATTERNS:
        if re.search(pattern, text):
            return True, "promo"
    
    return False, ""

def is_security_related(title: str, summary: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ —Å—Ç–∞—Ç—å—è –∫ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    text = f"{title} {summary}".lower()
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    security_keywords = [
        # –£–≥—Ä–æ–∑—ã
        "–≤–∏—Ä—É—Å", "–º–∞–ª–≤–∞—Ä", "—Ç—Ä–æ—è–Ω", "ransomware", "—à–∏—Ñ—Ä–æ–≤–∞–ª—å—â–∏–∫",
        "—Ñ–∏—à–∏–Ω–≥", "–º–æ—à–µ–Ω", "—É—Ç–µ—á–∫–∞", "–≤–∑–ª–æ–º", "—É—è–∑–≤–∏–º",
        "–≤—Ä–µ–¥–æ–Ω–æ—Å", "—à–ø–∏–æ–Ω", "—á–µ—Ä–≤—å", "—ç–∫—Å–ø–ª–æ–∏—Ç", "ddos",
        "–∫–∏–±–µ—Ä–∞—Ç–∞–∫", "–∫–∏–±–µ—Ä—É–≥—Ä–æ–∑", "—Ö–∞–∫–µ—Ä", "–∞—Ç–∞–∫",
        
        # –ó–∞—â–∏—Ç–∞
        "–ø–∞—Ä–æ–ª—å", "–¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä", "–∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫", "—à–∏—Ñ—Ä–æ–≤–∞–Ω",
        "vpn", "–∞–Ω—Ç–∏–≤–∏—Ä—É—Å", "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "–ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å",
        "–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω", "–∑–∞—â–∏—Ç", "–æ–±–Ω–æ–≤–ª–µ–Ω", "–ø–∞—Ç—á",
        
        # –ë–µ–ª—ã–π —Ö–∞–∫–∏–Ω–≥ / –≠—Ç–∏—á–Ω—ã–π —Ö–∞–∫–∏–Ω–≥
        "–ø–µ–Ω—Ç–µ—Å—Ç", "penetration test", "–±–∞–≥–±–∞—É–Ω—Ç–∏", "bug bounty",
        "—ç—Ç–∏—á–Ω", "ethical hack", "white hat", "–±–µ–ª–∞—è —à–ª—è–ø–∞",
        "–±–∞–≥—Ö–∞–Ω—Ç–µ—Ä", "–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "security researcher",
        "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω —Ä–∞—Å–∫—Ä—ã—Ç", "responsible disclosure",
        "vulnerability disclosure", "—Ä–∞—Å–∫—Ä—ã—Ç —É—è–∑–≤–∏–º–æ—Å—Ç",
        "—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "security testing",
        "—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω —É—è–∑–≤–∏–º–æ—Å—Ç", "vulnerability scan",
        "—Ä–µ–≤–µ—Ä—Å", "reverse engineering", "–∞–Ω–∞–ª–∏–∑ –∑–∞—â–∏—Ç",
        
        # –°–µ—Ä–≤–∏—Å—ã –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        "—Ç–µ–ª–µ—Ñ–æ–Ω", "—Å–º–∞—Ä—Ç—Ñ–æ–Ω", "android", "ios", "iphone",
        "–±—Ä–∞—É–∑–µ—Ä", "chrome", "firefox", "safari", "edge",
        "windows", "mac", "telegram", "whatsapp",
        "–∞–∫–∫–∞—É–Ω—Ç", "—É—á–µ—Ç–Ω", "google", "apple", "microsoft",
        
        # –î–∞–Ω–Ω—ã–µ
        "–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω", "–¥–∞–Ω–Ω—ã", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü", "cookie",
        "—Ç—Ä–µ–∫–∏–Ω–≥", "—Å–ª–µ–∂–∫", "–±–∏–æ–º–µ—Ç—Ä"
    ]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
    for keyword in security_keywords:
        if keyword in text:
            return True
    
    return False

# ============ STATE ============

class State:
    def __init__(self):
        self.data = {
            "posted_ids": {},
            "source_index": 0,
            "last_run": None,
        }
        self._load()
    
    def _load(self):
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE, "r", encoding="utf-8") as f:
                    self.data.update(json.load(f))
                print(f"üìÇ –ò—Å—Ç–æ—Ä–∏—è: {len(self.data['posted_ids'])} –ø–æ—Å—Ç–æ–≤")
            except:
                pass
    
    def save(self):
        self.data["last_run"] = datetime.now().isoformat()
        try:
            with open(STATE_FILE, "w", encoding="utf-8") as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
        except:
            pass
    
    def is_posted(self, article_id: str) -> bool:
        return article_id in self.data["posted_ids"]
    
    def mark_posted(self, article_id: str, source: str, title: str):
        self.data["posted_ids"][article_id] = {
            "ts": datetime.now().timestamp(),
            "source": source,
            "title": title[:100]
        }
        self.save()
    
    def cleanup_old(self):
        cutoff = datetime.now().timestamp() - (RETENTION_DAYS * 86400)
        self.data["posted_ids"] = {
            k: v for k, v in self.data["posted_ids"].items()
            if isinstance(v, dict) and v.get("ts", 0) > cutoff
        }
    
    def get_next_source_order(self) -> List[Dict]:
        idx = self.data["source_index"] % len(RSS_SOURCES)
        ordered = RSS_SOURCES[idx:] + RSS_SOURCES[:idx]
        self.data["source_index"] = (idx + 1) % len(RSS_SOURCES)
        return ordered

state = State()

# ============ –ü–ê–†–°–ò–ù–ì –ü–û–õ–ù–û–ì–û –¢–ï–ö–°–¢–ê ============

def fetch_full_article(url: str) -> Optional[str]:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω–æ–µ
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form']):
            tag.decompose()
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        content = None
        
        # Habr
        if 'habr.com' in url:
            content = soup.find('div', class_='tm-article-body')
        # SecurityLab
        elif 'securitylab.ru' in url:
            content = soup.find('div', class_='article-body') or soup.find('div', class_='news-body')
        # CNews
        elif 'cnews.ru' in url:
            content = soup.find('div', class_='news_container')
        # 3DNews
        elif '3dnews.ru' in url:
            content = soup.find('div', class_='article-entry')
        
        # –û–±—â–∏–π fallback
        if not content:
            content = soup.find('article') or soup.find('main') or soup.find('div', class_=re.compile(r'article|content|post|entry'))
        
        if content:
            text = content.get_text(separator='\n', strip=True)
            # –ß–∏—Å—Ç–∏–º
            text = re.sub(r'\n{3,}', '\n\n', text)
            text = re.sub(r'[ \t]+', ' ', text)
            return text[:4000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
        
        return None
        
    except Exception as e:
        print(f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {e}")
        return None

# ============ HELPERS ============

def clean_text(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'&\w+;', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def get_article_id(title: str, link: str) -> str:
    return hashlib.sha256(f"{title}|{link}".encode()).hexdigest()[:20]

def force_complete_sentence(text: str) -> str:
    text = text.strip()
    if not text:
        return text
    
    for pattern in [r'\s+–∏$', r'\s+–∞$', r'\s+–Ω–æ$', r'\s+—á—Ç–æ$', r':$', r';$', r',$']:
        text = re.sub(pattern, '', text)
    
    if text and text[-1] in '.!?':
        return text
    
    last_end = max(text.rfind('.'), text.rfind('!'), text.rfind('?'))
    if last_end > len(text) * 0.6:
        return text[:last_end + 1]
    
    return text + '.'

def get_hashtags(category: str) -> str:
    mapping = {
        "security": "#–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å #–∫–∏–±–µ—Ä—É–≥—Ä–æ–∑—ã",
        "ai": "#AI #–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ #—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        "tech": "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #IT",
        "tech_ru": "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–†–æ—Å—Å–∏—è #IT",
    }
    return mapping.get(category, "#—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏")

def build_final_post(text: str, link: str, category: str) -> str:
    text = force_complete_sentence(text.strip())
    hashtags = get_hashtags(category)
    source = f'\n\nüîó <a href="{link}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>'
    tags = f"\n\n{hashtags}"
    
    service_len = len(source) + len(tags) + 5
    max_text = TELEGRAM_CAPTION_LIMIT - service_len
    
    if len(text) > max_text:
        text = text[:max_text]
        last_end = max(text.rfind('. '), text.rfind('! '), text.rfind('? '))
        if last_end > max_text * 0.6:
            text = text[:last_end + 1]
    
    return text + tags + source

# ============ RSS LOADING ============

def load_rss(source: Dict) -> List[Dict]:
    articles = []
    
    try:
        resp = requests.get(source["url"], headers=HEADERS, timeout=20)
        feed = feedparser.parse(resp.content)
    except Exception as e:
        print(f"‚ùå {source['name']}: {e}")
        return []
    
    if not feed.entries:
        print(f"‚ö™ {source['name']}: –ø—É—Å—Ç–æ")
        return []
    
    now = datetime.now()
    max_age = timedelta(days=MAX_ARTICLE_AGE_DAYS)
    
    for entry in feed.entries[:30]:
        title = clean_text(entry.get("title", ""))
        link = entry.get("link", "")
        
        if not title or not link:
            continue
        
        article_id = get_article_id(title, link)
        
        if state.is_posted(article_id):
            continue
        
        pub_date = now
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            try:
                pub_date = datetime(*entry.published_parsed[:6])
            except:
                pass
        
        if now - pub_date > max_age:
            continue
        
        summary = clean_text(entry.get("summary", "") or entry.get("description", ""))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        excluded, reason = is_excluded(title, summary)
        if excluded:
            print(f"  ‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ ({reason}): {title[:50]}")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–µ–º—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        if not is_security_related(title, summary):
            print(f"  ‚è≠Ô∏è –ù–µ –ø–æ —Ç–µ–º–µ: {title[:50]}")
            continue
        
        articles.append({
            "id": article_id,
            "title": title,
            "summary": summary[:1500],
            "link": link,
            "source": source["name"],
            "category": source["category"],
            "date": pub_date,
        })
    
    if articles:
        print(f"‚úÖ {source['name']}: {len(articles)} —Å—Ç–∞—Ç–µ–π")
    
    return articles

# ============ TEXT GENERATION ============

async def generate_post_with_copilot_sdk(article: Dict) -> Optional[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–∞ —á–µ—Ä–µ–∑ Copilot SDK"""
    if not copilot_client:
        return None
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
        full_text = fetch_full_article(article["link"])
        content = full_text[:3000] if full_text else article["summary"]
        
        if full_text:
            print(f"  üìÑ –ü–æ–ª—É—á–µ–Ω –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        user_message = f"""{POST_FORMAT['template']}

---
–ò–°–¢–û–ß–ù–ò–ö:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article['title']}
–¢–µ–∫—Å—Ç: {content}
–°—Å—ã–ª–∫–∞: {article['link']}
---

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
1. –ü–∏—à–∏ –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
2. –ï—Å–ª–∏ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–µ –µ—Å—Ç—å —Ä–µ—à–µ–Ω–∏–µ ‚Äî –¥–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏
3. –ï—Å–ª–∏ —Ä–µ—à–µ–Ω–∏—è –Ω–µ—Ç ‚Äî –ø—Ä–æ—Å—Ç–æ –æ–±—ä—è—Å–Ω–∏ –ø—Ä–æ–±–ª–µ–º—É
4. –ù–ï –í–´–î–£–ú–´–í–ê–ô –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
"""
        
        # –°–æ–∑–¥–∞—ë–º —Å–µ—Å—Å–∏—é —Å –∞–≥–µ–Ω—Ç–æ–º
        session = copilot_client.create_session(
            system=POST_FORMAT["system"],
            temperature=0.6,
            max_tokens=900
        )
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
        response = await session.send_message(user_message)
        text = response.text.strip()
        
        # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏
        if text.startswith(('"', '¬´')) and text.endswith(('"', '¬ª')):
            text = text[1:-1].strip()
        
        if len(text) < 200:
            return None
        
        final = build_final_post(text, article["link"], article["category"])
        print(f"  ‚úÖ SDK: {len(final)} —Å–∏–º–≤–æ–ª–æ–≤")
        return final
        
    except Exception as e:
        print(f"  ‚ùå SDK –æ—à–∏–±–∫–∞: {e}")
        return None

def generate_post(article: Dict) -> Optional[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–∞ (—Å fallback –Ω–∞ OpenAI –µ—Å–ª–∏ SDK –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)"""
    
    # –ü—Ä–æ–±—É–µ–º Copilot SDK –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
    if copilot_client and USE_COPILOT_SDK:
        print("  ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Copilot SDK")
        result = asyncio.run(generate_post_with_copilot_sdk(article))
        if result:
            return result
        print("  ‚ö†Ô∏è SDK –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ OpenAI")
    
    # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π OpenAI
    full_text = fetch_full_article(article["link"])
    
    content_for_gpt = article["summary"]
    if full_text and len(full_text) > len(article["summary"]):
        content_for_gpt = full_text[:3000]
        print(f"  üìÑ –ü–æ–ª—É—á–µ–Ω –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    user_prompt = f"""{POST_FORMAT['template']}

---
–ò–°–¢–û–ß–ù–ò–ö:

–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article['title']}

–¢–µ–∫—Å—Ç: {content_for_gpt}

–°—Å—ã–ª–∫–∞: {article['link']}
---

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
1. –ü–∏—à–∏ –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
2. –ï—Å–ª–∏ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–µ –µ—Å—Ç—å —Ä–µ—à–µ–Ω–∏–µ ‚Äî –¥–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏
3. –ï—Å–ª–∏ —Ä–µ—à–µ–Ω–∏—è –Ω–µ—Ç ‚Äî –ø—Ä–æ—Å—Ç–æ –æ–±—ä—è—Å–Ω–∏ –ø—Ä–æ–±–ª–µ–º—É
4. –ù–ï –í–´–î–£–ú–´–í–ê–ô –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ç–∏–ø–∞ "–æ—Ç–∫—Ä–æ–π—Ç–µ App Store"
5. –õ—É—á—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Å—Ç –±–µ–∑ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, —á–µ–º –≤—ã–¥—É–º–∞–Ω–Ω—ã–µ —à–∞–≥–∏
"""

    for attempt in range(2):
        try:
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": POST_FORMAT["system"]},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.6,
                max_tokens=900,
            )
            
            text = response.choices[0].message.content.strip()
            
            # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏
            if text.startswith(('"', '¬´')) and text.endswith(('"', '¬ª')):
                text = text[1:-1].strip()
            
            if len(text) < 200:
                print(f"  ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ: {len(text)}")
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É—Å–æ—Ä–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã
            garbage_patterns = [
                r"–æ—Ç–∫—Ä–æ–π—Ç–µ app store",
                r"–æ—Ç–∫—Ä–æ–π—Ç–µ google play", 
                r"–∑–∞–π–¥–∏—Ç–µ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.*–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è",
                r"üíæ\s*—Å–æ—Ö—Ä–∞–Ω–∏",
                r"—á—Ç–æ –¥–µ–ª–∞—Ç—å:\s*\n\s*1\.\s*–æ–±–Ω–æ–≤–∏",
            ]
            
            is_garbage = any(re.search(p, text.lower()) for p in garbage_patterns)
            if is_garbage and "–æ–±–Ω–æ–≤–ª" not in article["summary"].lower():
                print(f"  üóëÔ∏è –í—ã–¥—É–º–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞")
                continue
            
            final = build_final_post(text, article["link"], article["category"])
            print(f"  ‚úÖ –ì–æ—Ç–æ–≤–æ: {len(final)} —Å–∏–º–≤–æ–ª–æ–≤")
            return final
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
            time.sleep(2)
    
    return None

# ============ IMAGE ============

def generate_image(title: str) -> Optional[str]:
    styles = [
        "modern tech illustration, clean, minimal",
        "futuristic digital art, blue tones",
        "abstract technology concept, professional",
    ]
    
    seed = random.randint(1, 999999999)
    keywords = re.sub(r'[^\w\s]', '', title)[:40]
    prompt = f"{random.choice(styles)}, {keywords}, no text, 4k"
    
    url = f"https://image.pollinations.ai/prompt/{urllib.parse.quote(prompt)}?seed={seed}&width=1024&height=1024&nologo=true"
    
    print(f"  üñº –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏...")
    
    try:
        resp = requests.get(url, timeout=60, headers=HEADERS)
        if resp.status_code == 200 and len(resp.content) > 10000:
            filename = f"img_{seed}.jpg"
            with open(filename, "wb") as f:
                f.write(resp.content)
            return filename
    except Exception as e:
        print(f"  ‚ö†Ô∏è –ö–∞—Ä—Ç–∏–Ω–∫–∞: {e}")
    
    return None

def cleanup_image(path: Optional[str]):
    if path and os.path.exists(path):
        try:
            os.remove(path)
        except:
            pass

# ============ MAIN ============

async def autopost():
    state.cleanup_old()
    print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π...\n")
    
    if copilot_client and USE_COPILOT_SDK:
        print("ü§ñ –†–µ–∂–∏–º: GitHub Copilot SDK")
    else:
        print("üîß –†–µ–∂–∏–º: OpenAI API")
    
    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    all_articles = []
    sources = state.get_next_source_order()
    
    for source in sources:
        articles = load_rss(source)
        all_articles.extend(articles)
    
    if not all_articles:
        print("\n‚ùå –ù–µ—Ç –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π")
        return
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
    all_articles.sort(key=lambda x: x["date"], reverse=True)
    
    print(f"\nüìä –í—Å–µ–≥–æ: {len(all_articles)} —Å—Ç–∞—Ç–µ–π")
    
    # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏
    for article in all_articles[:15]:
        print(f"\nüì∞ {article['title'][:60]}...")
        print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {article['source']}")
        
        post_text = generate_post(article)
        
        if not post_text:
            print("   ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue
        
        img = generate_image(article["title"])
        
        try:
            if img:
                await bot.send_photo(CHANNEL_ID, photo=FSInputFile(img), caption=post_text)
            else:
                await bot.send_message(CHANNEL_ID, text=post_text)
            
            state.mark_posted(article["id"], article["source"], article["title"])
            print("\n‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ!")
            return
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
        finally:
            cleanup_image(img)
    
    print("\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å")

async def main():
    try:
        await autopost()
    finally:
        await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())


import os
import json
import asyncio
import random
import re
import time
import hashlib
import html
import urllib.parse
import tempfile
import shutil
import logging
from dataclasses import dataclass, field
from typing import Literal, Optional
from difflib import SequenceMatcher

import aiohttp
import feedparser
from youtube_transcript_api import YouTubeTranscriptApi
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from groq import Groq, RateLimitError, APIError

# ============ –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ============

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("KiberSOS")

# ============ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ============

def get_env(name: str) -> str:
    val = os.getenv(name)
    if not val:
        logger.error(f"Missing: {name}")
        exit(1)
    return val

GROQ_API_KEY = get_env("GROQ_API_KEY")
TELEGRAM_BOT_TOKEN = get_env("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = get_env("CHANNEL_ID")

CACHE_DIR = os.getenv("CACHE_DIR", "cache_sec")
os.makedirs(CACHE_DIR, exist_ok=True)
STATE_FILE = os.path.join(CACHE_DIR, "state_groq_v2.json")

TEXT_ONLY_THRESHOLD = 700
MAX_POSTED_IDS = 400
HTTP_TIMEOUT = aiohttp.ClientTimeout(total=25)
IMAGE_TIMEOUT = aiohttp.ClientTimeout(total=40)

# ============ GROQ –õ–ò–ú–ò–¢–´ ============

@dataclass
class ModelConfig:
    name: str
    rpm: int  # requests per minute
    tpm: int  # tokens per minute
    daily_tokens: int
    priority: int  # –º–µ–Ω—å—à–µ = –≤—ã—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç

MODELS = {
    "heavy": ModelConfig("llama-3.3-70b-versatile", rpm=30, tpm=6000, daily_tokens=100000, priority=1),
    "light": ModelConfig("llama3-8b-8192", rpm=30, tpm=30000, daily_tokens=500000, priority=2),
    "fallback": ModelConfig("llama-3.1-8b-instant", rpm=30, tpm=20000, daily_tokens=500000, priority=3),
}

# ============ RATE LIMITER & TOKEN TRACKER ============

class GroqBudget:
    """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ª–∏–º–∏—Ç–æ–≤ Groq —Å –∞–≤—Ç–æ—Å–±—Ä–æ—Å–æ–º"""
    
    def __init__(self):
        self.state_file = os.path.join(CACHE_DIR, "groq_budget.json")
        self.data = self._load()
    
    def _load(self) -> dict:
        default = {
            "daily_tokens": {},  # model -> tokens used today
            "last_reset": time.strftime("%Y-%m-%d"),
            "last_request_time": {},  # model -> timestamp
            "request_count": {},  # model -> count in current minute
            "minute_start": {},  # model -> minute start timestamp
        }
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, "r") as f:
                    saved = json.load(f)
                    # –°–±—Ä–æ—Å –ø—Ä–∏ –Ω–æ–≤–æ–º –¥–Ω–µ
                    if saved.get("last_reset") != time.strftime("%Y-%m-%d"):
                        logger.info("üîÑ New day ‚Äî resetting token budget")
                        saved["daily_tokens"] = {}
                        saved["last_reset"] = time.strftime("%Y-%m-%d")
                    default.update(saved)
            except:
                pass
        return default
    
    def save(self):
        try:
            with open(self.state_file, "w") as f:
                json.dump(self.data, f)
        except Exception as e:
            logger.warning(f"Budget save error: {e}")
    
    def get_daily_usage(self, model: str) -> int:
        return self.data["daily_tokens"].get(model, 0)
    
    def add_tokens(self, model: str, tokens: int):
        self.data["daily_tokens"][model] = self.get_daily_usage(model) + tokens
        self.save()
    
    def can_use_model(self, model_key: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∏—Å—á–µ—Ä–ø–∞–Ω –ª–∏ –¥–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç"""
        if model_key not in MODELS:
            return False
        cfg = MODELS[model_key]
        used = self.get_daily_usage(cfg.name)
        remaining = cfg.daily_tokens - used
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º 10% —Ä–µ–∑–µ—Ä–≤
        if remaining < cfg.daily_tokens * 0.1:
            logger.warning(f"‚ö†Ô∏è {model_key} almost exhausted: {remaining} tokens left")
            return False
        return True
    
    async def wait_for_rate_limit(self, model_key: str):
        """–ñ–¥—ë–º, –µ—Å–ª–∏ –ø—Ä–µ–≤—ã—à–µ–Ω RPM"""
        cfg = MODELS[model_key]
        model = cfg.name
        now = time.time()
        
        # –°–±—Ä–æ—Å —Å—á—ë—Ç—á–∏–∫–∞ –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
        minute_start = self.data["minute_start"].get(model, 0)
        if now - minute_start > 60:
            self.data["minute_start"][model] = now
            self.data["request_count"][model] = 0
        
        count = self.data["request_count"].get(model, 0)
        
        if count >= cfg.rpm - 2:  # –û—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –≤ 2 –∑–∞–ø—Ä–æ—Å–∞
            wait_time = 60 - (now - minute_start) + 1
            logger.info(f"‚è≥ Rate limit {model_key}: waiting {wait_time:.1f}s")
            await asyncio.sleep(wait_time)
            self.data["minute_start"][model] = time.time()
            self.data["request_count"][model] = 0
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (2 —Å–µ–∫—É–Ω–¥—ã)
        last_req = self.data["last_request_time"].get(model, 0)
        if now - last_req < 2:
            await asyncio.sleep(2 - (now - last_req))
        
        self.data["request_count"][model] = self.data["request_count"].get(model, 0) + 1
        self.data["last_request_time"][model] = time.time()

budget = GroqBudget()

# ============ –§–ò–õ–¨–¢–†–´ (–ë–ï–ó AI) ============

STOP_WORDS = [
    "–Ω–∞—É—à–Ω–∏–∫", "jbl", "bluetooth", "–≥–∞—Ä–Ω–∏—Ç—É—Ä",
    "–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç", "–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º", "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥", "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è",
    "–º–≤—Å—Ñ–µ—Ä–∞", "–º—Å–≤—Å—Ñ–µ—Ä–∞", "astra linux", "–∞—Å—Ç—Ä–∞ –ª–∏–Ω—É–∫—Å", "red os", "—Ä–µ–¥ –æ—Å",
    "–∏–º–ø–æ—Ä—Ç–æ–∑–∞–º–µ—â", "postgresql", "highload", "golang", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç", "–∫–∞–∑–∏–Ω–æ"
]

BANNED_PHRASES = [
    "–∏–∑ –¥–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", "–∏–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤",
    "—Ä–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è–π—Ç–µ", "–±—É–¥—å—Ç–µ –±–¥–∏—Ç–µ–ª—å–Ω—ã", "–±—É–¥—å—Ç–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã",
    "–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ —á–∏—Ç–∞–π—Ç–µ", "–ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è", "–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–Ω—Ç–∏–≤–∏—Ä—É—Å",
    "–Ω–∞–¥—ë–∂–Ω—ã–π –ø–∞—Ä–æ–ª—å", "–Ω–∞–¥–µ–∂–Ω—ã–π –ø–∞—Ä–æ–ª—å", "—Å–ª–æ–∂–Ω—ã–π –ø–∞—Ä–æ–ª—å",
    "–Ω–µ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –ø–æ —Å—Å—ã–ª–∫–∞–º", "–Ω–µ –æ—Ç–∫—Ä—ã–≤–∞–π—Ç–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ",
    "–¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω—É—é –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é", "–¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è"
]

def local_similarity(title1: str, title2: str) -> float:
    """–õ–æ–∫–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –±–µ–∑ AI"""
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    t1 = re.sub(r'[^\w\s]', '', title1.lower())
    t2 = re.sub(r'[^\w\s]', '', title2.lower())
    
    # SequenceMatcher –¥–ª—è –æ–±—â–µ–π –ø–æ—Ö–æ–∂–µ—Å—Ç–∏
    ratio = SequenceMatcher(None, t1, t2).ratio()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    words1 = set(t1.split())
    words2 = set(t2.split())
    
    # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    stop = {'–≤', '–Ω–∞', '–∏', '–¥–ª—è', '—Å', '–ø–æ', '–∏–∑', '–∫', '–æ—Ç', 'the', 'a', 'an', 'in', 'on', 'for'}
    words1 -= stop
    words2 -= stop
    
    if not words1 or not words2:
        return ratio
    
    # Jaccard similarity –¥–ª—è —Å–ª–æ–≤
    intersection = len(words1 & words2)
    union = len(words1 | words2)
    jaccard = intersection / union if union else 0
    
    return max(ratio, jaccard)

def is_local_duplicate(title: str, recent_titles: list, threshold: float = 0.6) -> bool:
    """–õ–æ–∫–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ‚Äî —ç–∫–æ–Ω–æ–º–∏—Ç –≤—ã–∑–æ–≤—ã AI"""
    for recent in recent_titles[-30:]:
        if local_similarity(title, recent) > threshold:
            logger.info(f"üîÑ Local duplicate: '{title}' ~ '{recent}'")
            return True
    return False

def is_too_generic(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–∞–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
    text_lower = text.lower()
    count = sum(1 for phrase in BANNED_PHRASES if phrase in text_lower)
    return count >= 2

def passes_local_filters(title: str, text: str) -> bool:
    """–í—Å–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ"""
    low_title = title.lower()
    low_text = (text or "").lower()
    
    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
    for word in STOP_WORDS:
        if word in low_title or word in low_text:
            logger.info(f"üö´ Stop word '{word}': {title}")
            return False
    
    # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
    if len(text) < 100:
        logger.info(f"üö´ Too short: {title}")
        return False
    
    return True

# ============ GROQ API –° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ï–ô ============

async def call_groq(
    prompt: str, 
    model_preference: str = "heavy",
    max_tokens: int = 1500
) -> tuple[str, int]:
    """
    –í—ã–∑–æ–≤ Groq —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º —Ç–æ–∫–µ–Ω–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ_—Ç–æ–∫–µ–Ω–æ–≤)
    """
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è fallback
    model_order = ["heavy", "light", "fallback"]
    if model_preference == "light":
        model_order = ["light", "fallback", "heavy"]
    
    last_error = None
    
    for model_key in model_order:
        if not budget.can_use_model(model_key):
            continue
        
        cfg = MODELS[model_key]
        
        try:
            await budget.wait_for_rate_limit(model_key)
            
            response = await asyncio.to_thread(
                lambda: groq_client.chat.completions.create(
                    model=cfg.name,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens
                )
            )
            
            result = response.choices[0].message.content.strip()
            tokens_used = response.usage.total_tokens if response.usage else max_tokens
            
            budget.add_tokens(cfg.name, tokens_used)
            logger.debug(f"‚úì {model_key}: {tokens_used} tokens")
            
            return result, tokens_used
            
        except RateLimitError as e:
            logger.warning(f"‚ö†Ô∏è Rate limit {model_key}: {e}")
            last_error = e
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ
            await asyncio.sleep(30)
            continue
            
        except APIError as e:
            logger.warning(f"‚ö†Ô∏è API error {model_key}: {e}")
            last_error = e
            continue
            
        except Exception as e:
            logger.error(f"‚ùå Unexpected error {model_key}: {e}")
            last_error = e
            continue
    
    logger.error(f"‚ùå All models failed: {last_error}")
    return "", 0

# ============ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í ============

async def check_duplicate(new_title: str, recent_titles: list) -> bool:
    """
    –î–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –ª–æ–∫–∞–ª—å–Ω–æ, –ø–æ—Ç–æ–º AI (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    """
    if not recent_titles:
        return False
    
    # –≠—Ç–∞–ø 1: –õ–æ–∫–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ!)
    if is_local_duplicate(new_title, recent_titles):
        return True
    
    # –≠—Ç–∞–ø 2: AI –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
    # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ 10 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
    history = "\n".join(f"- {t}" for t in recent_titles[-10:])
    
    # –ö–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç = –º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤
    prompt = f"""–¢–µ–º—ã: 
{history}

–ù–æ–≤–∞—è: "{new_title}"

–î—É–±–ª–∏–∫–∞—Ç? YES/NO"""

    answer, tokens = await call_groq(prompt, model_preference="light", max_tokens=10)
    
    if not answer:
        # –ü—Ä–∏ –æ—à–∏–±–∫–µ API ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–ª—É—á—à–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å, —á–µ–º –ø–æ—Ç–µ—Ä—è—Ç—å)
        return False
    
    return "YES" in answer.upper()

# ============ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–û–°–¢–ê (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–†–û–ú–ü–¢) ============

async def generate_post(item) -> Optional[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —É–∫–æ—Ä–æ—á–µ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤"""
    
    # –°–æ–∫—Ä–∞—â–∞–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
    text_preview = item.text[:2000]  # –ë—ã–ª–æ 3000
    
    prompt = f"""–ö–∏–±–µ—Ä–±–µ–∑-–∫–∞–Ω–∞–ª. –ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ, —Å –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–æ–π.

–ù–û–í–û–°–¢–¨: {item.title}
{text_preview}

–ü–†–ê–í–ò–õ–ê:
- –ë–µ–∑ –±–∞–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π (–ø–∞—Ä–æ–ª–∏, –∞–Ω—Ç–∏–≤–∏—Ä—É—Å, "–±—É–¥—å—Ç–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã")
- –¢–æ–ª—å–∫–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —É–≥—Ä–æ–∑—ã –∏ –¥–µ–π—Å—Ç–≤–∏—è
- SKIP –µ—Å–ª–∏ –Ω–µ—Ç –ø–æ–ª—å–∑—ã –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —é–∑–µ—Ä–∞

–§–û–†–ú–ê–¢:
üî• [–ó–∞–≥–æ–ª–æ–≤–æ–∫]

[2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: —Å—É—Ç—å + –º–µ—Ö–∞–Ω–∏–∫–∞]

üëá –ß–¢–û –°–î–ï–õ–ê–¢–¨:
‚Ä¢ [–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ]

–ü–æ—Å—Ç –∏–ª–∏ SKIP:"""

    text, tokens = await call_groq(prompt, model_preference="heavy", max_tokens=800)
    
    logger.info(f"üìù Generated: {tokens} tokens")
    
    if not text or "SKIP" in text.upper() or len(text) < 100:
        return None
    
    if is_too_generic(text):
        logger.info(f"‚è© Too generic: {item.title}")
        return None
    
    return text + f"\n\nüîó <a href='{item.link}'>–ò—Å—Ç–æ—á–Ω–∏–∫</a>"

# ============ RSS SOURCES ============

RSS_SOURCES = [
    {"name": "Kaspersky Daily", "url": "https://www.kaspersky.ru/blog/feed/"},
    {"name": "Kod.ru", "url": "https://kod.ru/rss/"},
    {"name": "BleepingComputer", "url": "https://www.bleepingcomputer.com/feed/"},
    {"name": "Habr Security", "url": "https://habr.com/ru/rss/hub/infosecurity/all/?fl=ru"},
    {"name": "SecurityLab", "url": "https://www.securitylab.ru/rss/news/"},
]

YOUTUBE_CHANNELS = [
    {"name": "Overbafer1", "id": "UC-lHJ97lqoOGgsLFuQ8Y8_g"},
    {"name": "NetworkChuck", "id": "UC9x0AN7BWHpXyPic4IQC74Q"},
]

@dataclass
class NewsItem:
    type: Literal["news", "video"]
    title: str
    text: str
    link: str
    source: str
    uid: str

# ============ INIT ============

bot = Bot(token=TELEGRAM_BOT_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
groq_client = Groq(api_key=GROQ_API_KEY)

# ============ STATE ============

class State:
    def __init__(self):
        self.data = {"posted_ids": {}, "recent_titles": []}
        self._load()
    
    def _load(self):
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE, "r", encoding="utf-8") as f:
                    self.data.update(json.load(f))
                logger.info(f"üíæ Memory: {len(self.data.get('recent_titles', []))} topics")
            except: 
                pass
    
    def save(self):
        fd, tmp_path = tempfile.mkstemp(dir=CACHE_DIR, suffix='.json')
        try:
            with os.fdopen(fd, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, indent=2, ensure_ascii=False)
            shutil.move(tmp_path, STATE_FILE)
        except Exception as e:
            logger.error(f"Save error: {e}")
            if os.path.exists(tmp_path): 
                os.unlink(tmp_path)
    
    def is_posted(self, uid): 
        return uid in self.data["posted_ids"]
    
    def mark_posted(self, uid, title):
        if len(self.data["posted_ids"]) > MAX_POSTED_IDS:
            sorted_ids = sorted(self.data["posted_ids"].items(), key=lambda x: x[1])
            self.data["posted_ids"] = dict(sorted_ids[-300:])
        self.data["posted_ids"][uid] = int(time.time())
        self.data["recent_titles"].append(title)
        if len(self.data["recent_titles"]) > 40:
            self.data["recent_titles"] = self.data["recent_titles"][-40:]
        self.save()

    def get_recent_titles(self): 
        return self.data.get("recent_titles", [])

state = State()

# ============ UTILS ============

def clean_text(text):
    if not text: 
        return ""
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return html.unescape(text).strip()

# ============ FETCHERS ============

async def fetch_rss(source, session):
    items = []
    try:
        async with session.get(source['url'], timeout=HTTP_TIMEOUT) as resp:
            if resp.status != 200: 
                return []
            text = await resp.text()
        feed = feedparser.parse(text)
        for entry in feed.entries[:5]:
            link = entry.get('link')
            if not link: 
                continue
            uid = hashlib.md5(link.encode()).hexdigest()
            if state.is_posted(uid): 
                continue
            
            title = entry.get('title', '')
            text = clean_text(entry.get("summary", ""))
            
            # –õ–æ–∫–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –î–û –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
            if not passes_local_filters(title, text):
                continue
                
            items.append(NewsItem(
                type="news", title=title,
                text=text, link=link, 
                source=source['name'], uid=uid
            ))
    except Exception as e:
        logger.warning(f"RSS error {source['name']}: {e}")
    return items

async def fetch_youtube(channel, session):
    items = []
    try:
        url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel['id']}"
        async with session.get(url, timeout=HTTP_TIMEOUT) as resp:
            if resp.status != 200: 
                return []
            text = await resp.text()
        feed = feedparser.parse(text)
        for entry in feed.entries[:2]:
            vid = entry.get('yt_videoid')
            uid = f"yt_{vid}"
            if state.is_posted(uid): 
                continue
            try:
                # –Ø–≤–Ω–æ –ø–µ—Ä–µ–¥–∞—ë–º vid –≤ –ª—è–º–±–¥—É
                transcript = await asyncio.to_thread(
                    lambda v=vid: YouTubeTranscriptApi.list_transcripts(v)
                        .find_transcript(['ru', 'en']).fetch()
                )
                full_text = " ".join([t['text'] for t in transcript])
                
                if not passes_local_filters(entry.title, full_text):
                    continue
                    
                items.append(NewsItem(
                    type="video", title=entry.title, 
                    text=full_text[:5000],
                    link=entry.link, 
                    source=f"YouTube {channel['name']}", 
                    uid=uid
                ))
            except: 
                pass
    except Exception as e:
        logger.warning(f"YT error {channel['name']}: {e}")
    return items

# ============ IMAGES ============

async def generate_image(title, session):
    try:
        styles = ["cyberpunk neon", "matrix code", "glitch art"]
        clean_t = re.sub(r'[^a-zA-Z0-9\s]', '', title)[:40]
        prompt = f"hacker silhouette, {clean_t}, {random.choice(styles)}"
        encoded = urllib.parse.quote(prompt)
        seed = random.randint(0, 99999)
        url = f"https://image.pollinations.ai/prompt/{encoded}?width=1280&height=720&nologo=true&seed={seed}"
        
        async with session.get(url, timeout=IMAGE_TIMEOUT) as resp:
            if resp.status == 200:
                data = await resp.read()
                if len(data) > 1000:
                    path = os.path.join(CACHE_DIR, f"img_{int(time.time())}.jpg")
                    with open(path, "wb") as f: 
                        f.write(data)
                    return path
    except Exception as e:
        logger.warning(f"Image error: {e}")
    return None

# ============ MAIN ============

async def main():
    logger.info("üöÄ Starting (Groq Optimized v2)...")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π –±—é–¥–∂–µ—Ç
    for key, cfg in MODELS.items():
        used = budget.get_daily_usage(cfg.name)
        remaining = cfg.daily_tokens - used
        pct = (remaining / cfg.daily_tokens) * 100
        logger.info(f"üí∞ {key}: {remaining:,} tokens left ({pct:.1f}%)")
    
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_rss(s, session) for s in RSS_SOURCES]
        tasks += [fetch_youtube(c, session) for c in YOUTUBE_CHANNELS]
        results = await asyncio.gather(*tasks)
        all_items = [item for sublist in results for item in sublist]
        
        logger.info(f"üì¶ Found {len(all_items)} items (after local filters)")
        random.shuffle(all_items)
        
        posts_today = 0
        max_posts = 3  # –õ–∏–º–∏—Ç –ø–æ—Å—Ç–æ–≤ –∑–∞ –∑–∞–ø—É—Å–∫
        
        for item in all_items:
            if posts_today >= max_posts:
                logger.info(f"üìä Reached post limit ({max_posts})")
                break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±—é–¥–∂–µ—Ç –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            if not budget.can_use_model("light"):
                logger.warning("‚ö†Ô∏è Token budget exhausted!")
                break
            
            logger.info(f"üîç Checking: {item.title}")
            
            # –î–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            if await check_duplicate(item.title, state.get_recent_titles()):
                state.mark_posted(item.uid, item.title)
                continue
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–∞
            post_text = await generate_post(item)
            if not post_text:
                state.mark_posted(item.uid, item.title)
                continue
            
            try:
                if len(post_text) > TEXT_ONLY_THRESHOLD:
                    await bot.send_message(CHANNEL_ID, text=post_text)
                else:
                    img = await generate_image(item.title, session)
                    if img:
                        await bot.send_photo(
                            CHANNEL_ID, 
                            photo=FSInputFile(img), 
                            caption=post_text
                        )
                        os.remove(img)
                    else:
                        await bot.send_message(CHANNEL_ID, text=post_text)
                
                logger.info("‚úÖ Posted!")
                state.mark_posted(item.uid, item.title)
                posts_today += 1
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ—Å—Ç–∞–º–∏
                await asyncio.sleep(5)
                
            except Exception as e:
                logger.error(f"Telegram error: {e}")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info("üìä Session stats:")
    for key, cfg in MODELS.items():
        used = budget.get_daily_usage(cfg.name)
        logger.info(f"   {key}: {used:,} tokens used today")
    
    await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())

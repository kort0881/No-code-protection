import os
import json
import asyncio
import random
import re
import time
import hashlib
import html
import urllib.parse
import tempfile
import shutil
import logging
from dataclasses import dataclass
from typing import Literal, Optional
from difflib import SequenceMatcher

import aiohttp
import feedparser
from youtube_transcript_api import YouTubeTranscriptApi
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.types import FSInputFile
from groq import Groq, RateLimitError, APIError

# ============ –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ============

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("KiberSOS")

# ============ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ============

def get_env(name: str) -> str:
    val = os.getenv(name)
    if not val:
        logger.error(f"Missing: {name}")
        exit(1)
    return val

GROQ_API_KEY = get_env("GROQ_API_KEY")
TELEGRAM_BOT_TOKEN = get_env("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = get_env("CHANNEL_ID")

CACHE_DIR = os.getenv("CACHE_DIR", "cache_sec")
os.makedirs(CACHE_DIR, exist_ok=True)
STATE_FILE = os.path.join(CACHE_DIR, "state_groq_v2.json")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
TEXT_ONLY_THRESHOLD = 700  # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω–µ–µ, –∫–∞—Ä—Ç–∏–Ω–∫—É –ù–ï –¥–µ–ª–∞–µ–º
MAX_POSTED_IDS = 400
HTTP_TIMEOUT = aiohttp.ClientTimeout(total=25)
IMAGE_TIMEOUT = aiohttp.ClientTimeout(total=40)

# ============ GROQ –õ–ò–ú–ò–¢–´ (–ë–Æ–î–ñ–ï–¢) ============

@dataclass
class ModelConfig:
    name: str
    rpm: int  # requests per minute
    tpm: int  # tokens per minute
    daily_tokens: int
    priority: int

MODELS = {
    "heavy": ModelConfig("llama-3.3-70b-versatile", rpm=30, tpm=6000, daily_tokens=100000, priority=1),
    "light": ModelConfig("llama3-8b-8192", rpm=30, tpm=30000, daily_tokens=500000, priority=2),
    "fallback": ModelConfig("llama-3.1-8b-instant", rpm=30, tpm=20000, daily_tokens=500000, priority=3),
}

class GroqBudget:
    """–£–º–Ω–æ–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ª–∏–º–∏—Ç–æ–≤ Groq"""
    
    def __init__(self):
        self.state_file = os.path.join(CACHE_DIR, "groq_budget.json")
        self.data = self._load()
    
    def _load(self) -> dict:
        default = {
            "daily_tokens": {},
            "last_reset": time.strftime("%Y-%m-%d"),
            "last_request_time": {},
            "request_count": {},
            "minute_start": {},
        }
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, "r") as f:
                    saved = json.load(f)
                    if saved.get("last_reset") != time.strftime("%Y-%m-%d"):
                        logger.info("üîÑ –ù–æ–≤—ã–π –¥–µ–Ω—å ‚Äî —Å–±—Ä–æ—Å –ª–∏–º–∏—Ç–æ–≤ Groq")
                        saved["daily_tokens"] = {}
                        saved["last_reset"] = time.strftime("%Y-%m-%d")
                    default.update(saved)
            except: pass
        return default
    
    def save(self):
        try:
            with open(self.state_file, "w") as f: json.dump(self.data, f)
        except: pass
    
    def add_tokens(self, model: str, tokens: int):
        self.data["daily_tokens"][model] = self.data["daily_tokens"].get(model, 0) + tokens
        self.save()
    
    def can_use_model(self, model_key: str) -> bool:
        if model_key not in MODELS: return False
        cfg = MODELS[model_key]
        used = self.data["daily_tokens"].get(cfg.name, 0)
        return (cfg.daily_tokens - used) > (cfg.daily_tokens * 0.05) # 5% —Ä–µ–∑–µ—Ä–≤
    
    async def wait_for_rate_limit(self, model_key: str):
        cfg = MODELS[model_key]
        model = cfg.name
        now = time.time()
        
        # –°–±—Ä–æ—Å –º–∏–Ω—É—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞
        if now - self.data["minute_start"].get(model, 0) > 60:
            self.data["minute_start"][model] = now
            self.data["request_count"][model] = 0
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ RPM
        if self.data["request_count"].get(model, 0) >= cfg.rpm - 2:
            wait = 60 - (now - self.data["minute_start"][model]) + 1
            logger.info(f"‚è≥ –õ–∏–º–∏—Ç RPM ({model_key}). –ñ–¥–µ–º {wait:.1f}—Å")
            await asyncio.sleep(wait)
            self.data["minute_start"][model] = time.time()
            self.data["request_count"][model] = 0
            
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–∞–Ω—Ç–∏-—Å–ø–∞–º)
        last = self.data["last_request_time"].get(model, 0)
        if now - last < 2: await asyncio.sleep(2)
        
        self.data["request_count"][model] = self.data["request_count"].get(model, 0) + 1
        self.data["last_request_time"][model] = time.time()

budget = GroqBudget()

# ============ –§–ò–õ–¨–¢–†–´ ============

STOP_WORDS = [
    "–Ω–∞—É—à–Ω–∏–∫", "jbl", "bluetooth", "–≥–∞—Ä–Ω–∏—Ç—É—Ä",
    "–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç", "–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º", "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥", "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è",
    "–º–≤—Å—Ñ–µ—Ä–∞", "–º—Å–≤—Å—Ñ–µ—Ä–∞", "astra linux", "–∞—Å—Ç—Ä–∞ –ª–∏–Ω—É–∫—Å", "red os", "—Ä–µ–¥ –æ—Å",
    "–∏–º–ø–æ—Ä—Ç–æ–∑–∞–º–µ—â", "postgresql", "highload", "golang", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç", "–∫–∞–∑–∏–Ω–æ"
]

BANNED_PHRASES = [
    "–∏–∑ –¥–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", "—Ä–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è–π—Ç–µ", "–±—É–¥—å—Ç–µ –±–¥–∏—Ç–µ–ª—å–Ω—ã",
    "–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ —á–∏—Ç–∞–π—Ç–µ", "–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–Ω—Ç–∏–≤–∏—Ä—É—Å", "–Ω–∞–¥—ë–∂–Ω—ã–π –ø–∞—Ä–æ–ª—å",
    "–Ω–µ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –ø–æ —Å—Å—ã–ª–∫–∞–º"
]

def is_too_generic(text: str) -> bool:
    """–ï—Å–ª–∏ –ø–æ—Å—Ç —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –±–∞–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π ‚Äî –≤—ã–∫–∏–¥—ã–≤–∞–µ–º"""
    text_lower = text.lower()
    count = sum(1 for phrase in BANNED_PHRASES if phrase in text_lower)
    return count >= 2

def passes_local_filters(title: str, text: str) -> bool:
    """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ AI"""
    content = (title + " " + text).lower()
    if any(w in content for w in STOP_WORDS):
        logger.info(f"üö´ Stop word found: {title}")
        return False
    if len(text) < 100:
        return False
    return True

# ============ GROQ CALLER ============

async def call_groq(prompt: str, model_pref: str = "heavy", max_tokens: int = 1500) -> tuple[str, int]:
    """–£–º–Ω—ã–π –≤—ã–∑–æ–≤ —Å –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
    order = ["heavy", "light", "fallback"] if model_pref == "heavy" else ["light", "fallback", "heavy"]
    
    for key in order:
        if not budget.can_use_model(key): continue
        cfg = MODELS[key]
        
        try:
            await budget.wait_for_rate_limit(key)
            response = await asyncio.to_thread(
                lambda: groq_client.chat.completions.create(
                    model=cfg.name,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens
                )
            )
            res = response.choices[0].message.content.strip()
            tokens = response.usage.total_tokens if response.usage else 0
            budget.add_tokens(cfg.name, tokens)
            return res, tokens
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error {key}: {e}")
            await asyncio.sleep(5)
            continue
            
    return "", 0

# ============ –õ–û–ì–ò–ö–ê ============

async def check_duplicate(new_title: str, recent: list) -> bool:
    """–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ), –ø–æ—Ç–æ–º AI"""
    if not recent: return False
    
    # 1. –õ–æ–∫–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (SequenceMatcher)
    norm_new = re.sub(r'\W', '', new_title.lower())
    for old in recent[-20:]:
        norm_old = re.sub(r'\W', '', old.lower())
        if SequenceMatcher(None, norm_new, norm_old).ratio() > 0.6:
            logger.info(f"üîÑ Local duplicate: {new_title}")
            return True
            
    # 2. AI –ø—Ä–æ–≤–µ—Ä–∫–∞ (–ª–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å)
    history = "\n".join(f"- {t}" for t in recent[-10:])
    prompt = f"–¢–µ–º—ã:\n{history}\n\n–ù–æ–≤–∞—è: '{new_title}'\n–î—É–±–ª–∏–∫–∞—Ç? YES/NO"
    ans, _ = await call_groq(prompt, "light", 10)
    
    return "YES" in ans.upper()

async def generate_post(item) -> Optional[str]:
    prompt = f"""–ö–∏–±–µ—Ä–±–µ–∑-–∫–∞–Ω–∞–ª. –ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ, —Å –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–æ–π.

–ù–û–í–û–°–¢–¨: {item.title}
{item.text[:2000]}

–ü–†–ê–í–ò–õ–ê:
- –ë–µ–∑ –±–∞–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π (–ø–∞—Ä–æ–ª–∏, –∞–Ω—Ç–∏–≤–∏—Ä—É—Å, "–±—É–¥—å—Ç–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã")
- –¢–æ–ª—å–∫–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —É–≥—Ä–æ–∑—ã –∏ –¥–µ–π—Å—Ç–≤–∏—è
- SKIP –µ—Å–ª–∏ –Ω–µ—Ç –ø–æ–ª—å–∑—ã –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —é–∑–µ—Ä–∞

–§–û–†–ú–ê–¢:
üî• [–ó–∞–≥–æ–ª–æ–≤–æ–∫]

[2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: —Å—É—Ç—å + –º–µ—Ö–∞–Ω–∏–∫–∞]

üëá –ß–¢–û –°–î–ï–õ–ê–¢–¨:
‚Ä¢ [–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ]

–ü–æ—Å—Ç –∏–ª–∏ SKIP:"""

    text, _ = await call_groq(prompt, "heavy", 1000)
    
    if not text or "SKIP" in text.upper() or len(text) < 100:
        return None
    if is_too_generic(text):
        logger.info(f"‚è© Too generic: {item.title}")
        return None
        
    return text + f"\n\nüîó <a href='{item.link}'>–ò—Å—Ç–æ—á–Ω–∏–∫</a>"

# ============ –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø ============

async def generate_image(title, session):
    try:
        styles = ["cyberpunk neon", "matrix code", "glitch art", "isometric 3d"]
        clean_t = re.sub(r'[^a-zA-Z0-9\s]', '', title)[:40]
        prompt = f"hacker silhouette, {clean_t}, {random.choice(styles)}"
        encoded = urllib.parse.quote(prompt)
        url = f"https://image.pollinations.ai/prompt/{encoded}?width=1280&height=720&nologo=true&seed={random.randint(0,99999)}"
        
        async with session.get(url, timeout=IMAGE_TIMEOUT) as resp:
            if resp.status == 200:
                data = await resp.read()
                if len(data) > 5000:
                    path = os.path.join(CACHE_DIR, f"img_{int(time.time())}.jpg")
                    with open(path, "wb") as f: f.write(data)
                    return path
    except: pass
    return None

# ============ –ö–õ–ê–°–°–´ –ò –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ============

@dataclass
class NewsItem:
    type: Literal["news", "video"]
    title: str
    text: str
    link: str
    source: str
    uid: str

bot = Bot(token=TELEGRAM_BOT_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
groq_client = Groq(api_key=GROQ_API_KEY)

# ============ STATE (–ü–ê–ú–Ø–¢–¨) ============

class State:
    def __init__(self):
        self.data = {"posted_ids": {}, "recent_titles": []}
        self._load()
    
    def _load(self):
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE, "r", encoding="utf-8") as f: self.data.update(json.load(f))
            except: pass
    
    def save(self):
        fd, tmp = tempfile.mkstemp(dir=CACHE_DIR, suffix='.json')
        try:
            with os.fdopen(fd, 'w', encoding='utf-8') as f: json.dump(self.data, f)
            shutil.move(tmp, STATE_FILE)
        except: os.unlink(tmp)
    
    def is_posted(self, uid): return uid in self.data["posted_ids"]
    
    def mark_posted(self, uid, title):
        if len(self.data["posted_ids"]) > MAX_POSTED_IDS:
            self.data["posted_ids"] = dict(sorted(self.data["posted_ids"].items(), key=lambda x: x[1])[-300:])
        self.data["posted_ids"][uid] = int(time.time())
        self.data["recent_titles"].append(title)
        if len(self.data["recent_titles"]) > 40: self.data["recent_titles"] = self.data["recent_titles"][-40:]
        self.save()

state = State()

# ============ –°–ë–û–†–©–ò–ö–ò ============

async def fetch_rss(source, session):
    items = []
    try:
        async with session.get(source['url'], timeout=HTTP_TIMEOUT) as resp:
            if resp.status != 200: return []
            text = await resp.text()
        feed = feedparser.parse(text)
        for entry in feed.entries[:5]:
            link = entry.get('link')
            if not link: continue
            uid = hashlib.md5(link.encode()).hexdigest()
            if state.is_posted(uid): continue
            
            title = entry.get('title', '')
            text = clean_text(entry.get("summary", ""))
            
            if passes_local_filters(title, text):
                items.append(NewsItem("news", title, text, link, source['name'], uid))
    except: pass
    return items

async def fetch_youtube(channel, session):
    items = []
    try:
        url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel['id']}"
        async with session.get(url, timeout=HTTP_TIMEOUT) as resp:
            if resp.status != 200: return []
            text = await resp.text()
        feed = feedparser.parse(text)
        for entry in feed.entries[:2]:
            vid = entry.get('yt_videoid')
            uid = f"yt_{vid}"
            if state.is_posted(uid): continue
            try:
                ts = await asyncio.to_thread(lambda: YouTubeTranscriptApi.list_transcripts(vid).find_transcript(['ru', 'en']).fetch())
                full = " ".join([t['text'] for t in ts])
                if passes_local_filters(entry.title, full):
                    items.append(NewsItem("video", entry.title, full[:5000], entry.link, f"YouTube {channel['name']}", uid))
            except: pass
    except: pass
    return items

def clean_text(text):
    if not text: return ""
    return html.unescape(re.sub(r'<[^>]+>', ' ', text)).strip()

# ============ MAIN ============

async def main():
    logger.info("üöÄ Starting (1 POST LIMIT MODE)...")
    
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_rss(s, session) for s in RSS_SOURCES] + [fetch_youtube(c, session) for c in YOUTUBE_CHANNELS]
        results = await asyncio.gather(*tasks)
        all_items = [i for r in results for i in r]
        
        logger.info(f"üì¶ Found {len(all_items)} items")
        random.shuffle(all_items)
        
        posts_done = 0
        MAX_POSTS_PER_RUN = 1  # <--- –í–û–¢ –ì–õ–ê–í–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï
        
        for item in all_items:
            if posts_done >= MAX_POSTS_PER_RUN:
                break
            
            if not budget.can_use_model("light"):
                logger.warning("‚ö†Ô∏è Daily budget exhausted")
                break
            
            logger.info(f"üîç Analyzing: {item.title}")
            
            if await check_duplicate(item.title, state.data["recent_titles"]):
                state.mark_posted(item.uid, item.title)
                continue
            
            post_text = await generate_post(item)
            if not post_text:
                state.mark_posted(item.uid, item.title)
                continue
            
            try:
                # –†–µ—à–µ–Ω–∏–µ: –¢–µ–∫—Å—Ç –∏–ª–∏ –ö–∞—Ä—Ç–∏–Ω–∫–∞?
                if len(post_text) > TEXT_ONLY_THRESHOLD:
                    logger.info("üìú Text only (Long read)")
                    await bot.send_message(CHANNEL_ID, text=post_text)
                else:
                    logger.info("üì∏ Generating image...")
                    img = await generate_image(item.title, session)
                    if img:
                        await bot.send_photo(CHANNEL_ID, photo=FSInputFile(img), caption=post_text)
                        os.remove(img)
                    else:
                        await bot.send_message(CHANNEL_ID, text=post_text)
                
                logger.info("‚úÖ Posted successfully!")
                state.mark_posted(item.uid, item.title)
                posts_done += 1
                
            except Exception as e:
                logger.error(f"Telegram Error: {e}")

    await bot.session.close()

if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    RSS_SOURCES = [
        {"name": "Kaspersky", "url": "https://www.kaspersky.ru/blog/feed/"},
        {"name": "Kod.ru", "url": "https://kod.ru/rss/"},
        {"name": "BleepingComputer", "url": "https://www.bleepingcomputer.com/feed/"},
        {"name": "Habr Security", "url": "https://habr.com/ru/rss/hub/infosecurity/all/?fl=ru"},
        {"name": "SecurityLab", "url": "https://www.securitylab.ru/rss/news/"},
    ]
    YOUTUBE_CHANNELS = [
        {"name": "Overbafer1", "id": "UC-lHJ97lqoOGgsLFuQ8Y8_g"},
        {"name": "NetworkChuck", "id": "UC9x0AN7BWHpXyPic4IQC74Q"},
    ]
    
    asyncio.run(main())
